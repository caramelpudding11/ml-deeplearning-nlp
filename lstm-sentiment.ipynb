{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 22:55:13.100724: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-07 22:55:13.129468: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-07 22:55:13.337573: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-07 22:55:13.339682: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-07 22:55:14.221053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Library Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/prathiba/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/prathiba/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/prathiba/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the required NLTK data\n",
    "nltk.download('punkt')  # Punkt Tokenizer Model\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "nltk.download('wordnet')  # WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('EcoPreprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>polarity</th>\n",
       "      <th>division</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3870</td>\n",
       "      <td>able play youtube alexa</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>able recognize indian accent really well drop ...</td>\n",
       "      <td>0.2794</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487</td>\n",
       "      <td>absolute smart device amazon connect external ...</td>\n",
       "      <td>0.1827</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3204</td>\n",
       "      <td>absolutely amaze new member family control hom...</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1265</td>\n",
       "      <td>absolutely amaze previously sceptical invest m...</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review  polarity  \\\n",
       "0        3870                            able play youtube alexa    0.5000   \n",
       "1          62  able recognize indian accent really well drop ...    0.2794   \n",
       "2         487  absolute smart device amazon connect external ...    0.1827   \n",
       "3        3204  absolutely amaze new member family control hom...    0.3682   \n",
       "4        1265  absolutely amaze previously sceptical invest m...    0.2333   \n",
       "\n",
       "   division  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  positive  \n",
       "4  positive  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset='review')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the reviews\n",
    "df['review'] = df['review'].str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "df['review'] = df['review'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Tokenize the reviews\n",
    "df['review'] = df['review'].apply(word_tokenize)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['review'] = df['review'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Lemmatize the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['review'] = df['review'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of most frequent words to consider\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['review'].values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['review'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiments to one-hot vectors\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df['division'])\n",
    "Y = encoder.transform(df['division'])\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "39/39 [==============================] - 8s 150ms/step - loss: 0.5158 - accuracy: 0.7077 - val_loss: 0.4439 - val_accuracy: 0.7338\n",
      "Epoch 2/5\n",
      "39/39 [==============================] - 6s 152ms/step - loss: 0.4424 - accuracy: 0.7245 - val_loss: 0.4387 - val_accuracy: 0.7338\n",
      "Epoch 3/5\n",
      "39/39 [==============================] - 6s 151ms/step - loss: 0.3779 - accuracy: 0.7506 - val_loss: 0.3370 - val_accuracy: 0.7770\n",
      "Epoch 4/5\n",
      "39/39 [==============================] - 6s 154ms/step - loss: 0.2509 - accuracy: 0.8480 - val_loss: 0.2822 - val_accuracy: 0.8058\n",
      "Epoch 5/5\n",
      "39/39 [==============================] - 6s 153ms/step - loss: 0.1773 - accuracy: 0.9054 - val_loss: 0.2718 - val_accuracy: 0.8417\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='sigmoid'))  # 3 is the number of sentiment classes\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 18ms/step - loss: 0.2748 - accuracy: 0.8196\n",
      "Test set\n",
      "  Loss: 0.275\n",
      "  Accuracy: 0.820\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 19s]\n",
      "val_accuracy: 0.7873873710632324\n",
      "\n",
      "Best val_accuracy So Far: 0.8306306600570679\n",
      "Total elapsed time: 00h 01m 28s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Best Hyperparameters:\n",
      "<keras_tuner.engine.hyperparameters.hyperparameters.HyperParameters object at 0x7f32e52b9c10>\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 8s 91ms/step - loss: 0.7946 - accuracy: 0.7217 - val_loss: 0.6825 - val_accuracy: 0.7297\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 6s 86ms/step - loss: 0.5236 - accuracy: 0.7785 - val_loss: 0.5041 - val_accuracy: 0.7748\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 6s 84ms/step - loss: 0.3356 - accuracy: 0.8823 - val_loss: 0.4879 - val_accuracy: 0.8144\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 6s 84ms/step - loss: 0.2234 - accuracy: 0.9269 - val_loss: 0.4800 - val_accuracy: 0.8342\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 6s 85ms/step - loss: 0.1437 - accuracy: 0.9576 - val_loss: 0.5871 - val_accuracy: 0.8378\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 6s 84ms/step - loss: 0.0897 - accuracy: 0.9770 - val_loss: 0.5432 - val_accuracy: 0.8432\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 6s 88ms/step - loss: 0.0699 - accuracy: 0.9815 - val_loss: 0.5689 - val_accuracy: 0.8378\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 250, 96)           4800000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 96)                74112     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 291       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4874403 (18.59 MB)\n",
      "Trainable params: 4874403 (18.59 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "max_features = 50000\n",
    "maxlen = 200\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, hp.Int('embedding_dim', min_value=32, max_value=128, step=32), input_length=X.shape[1]))\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=32, max_value=128, step=32), dropout=hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    model.add(Dense(3, activation=hp.Choice('dense_activation',values=['relu','sigmoid','softmax']),kernel_initializer='he_normal'))\n",
    "    model.compile(optimizer='adam', loss=hp.Choice('loss_fn',values=['binary_crossentropy','categorical_crossentropy']), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize Keras Tuner RandomSearch\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    directory='tuner_dir',\n",
    "    project_name='lstm_sentiment'\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(X_train, Y_train, validation_split=0.2, epochs=3)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hps)\n",
    "\n",
    "# Build the final model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(X_train, Y_train, validation_split=0.2, epochs=10, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen embedding dimension: 96\n",
      "Chosen number of LSTM units: 96\n",
      "Chosen dropout rate: 0.2\n",
      "Chosen Activation Function: softmax\n",
      "Chosen Loss Function: categorical_crossentropy\n"
     ]
    }
   ],
   "source": [
    "# Print the chosen activation function and loss function\n",
    "best_activation = best_hps.get('dense_activation')\n",
    "best_loss_function = best_hps.get('loss_fn')\n",
    "best_em_dim = best_hps.get('embedding_dim')\n",
    "best_units = best_hps.get('lstm_units')\n",
    "best_dropout = best_hps.get('Dropout_rate')\n",
    "print(\"Chosen embedding dimension:\", best_em_dim)\n",
    "print(\"Chosen number of LSTM units:\", best_units)\n",
    "print(\"Chosen dropout rate:\", best_dropout)\n",
    "print(\"Chosen Activation Function:\", best_activation)\n",
    "print(\"Chosen Loss Function:\", best_loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 18ms/step - loss: 0.4911 - accuracy: 0.8355\n",
      "Test set\n",
      "  Loss: 0.491\n",
      "  Accuracy: 0.835\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.29      0.40        89\n",
      "     neutral       0.64      0.59      0.62        95\n",
      "    positive       0.86      0.95      0.90       509\n",
      "\n",
      "    accuracy                           0.82       693\n",
      "   macro avg       0.72      0.61      0.64       693\n",
      "weighted avg       0.80      0.82      0.80       693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your labels based on the order of your sentiment classes\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(Y_test.argmax(axis=1), Y_pred.argmax(axis=1), target_names=labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sentiment:  negative\n"
     ]
    }
   ],
   "source": [
    "# Assuming that 'text_to_predict' is the text for which you want to predict the sentiment\n",
    "text_to_predict = [\"The product is bad\"]\n",
    "sequences = tokenizer.texts_to_sequences(text_to_predict)\n",
    "padded = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(padded)\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "print(\"Predicted sentiment: \", labels[np.argmax(pred)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
