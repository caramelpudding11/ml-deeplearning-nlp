{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 17:16:10.989582: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-11 17:16:10.990921: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-11 17:16:11.023939: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-11 17:16:11.024672: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-11 17:16:11.585496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import string\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataline</th>\n",
       "      <th>Play</th>\n",
       "      <th>PlayerLinenumber</th>\n",
       "      <th>ActSceneLine</th>\n",
       "      <th>Player</th>\n",
       "      <th>PlayerLine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACT I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SCENE I. London. The palace.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enter KING HENRY, LORD JOHN OF LANCASTER, the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1.1</td>\n",
       "      <td>KING HENRY IV</td>\n",
       "      <td>So shaken as we are, so wan with care,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1.2</td>\n",
       "      <td>KING HENRY IV</td>\n",
       "      <td>Find we a time for frighted peace to pant,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataline      Play  PlayerLinenumber ActSceneLine         Player  \\\n",
       "0         1  Henry IV               NaN          NaN            NaN   \n",
       "1         2  Henry IV               NaN          NaN            NaN   \n",
       "2         3  Henry IV               NaN          NaN            NaN   \n",
       "3         4  Henry IV               1.0        1.1.1  KING HENRY IV   \n",
       "4         5  Henry IV               1.0        1.1.2  KING HENRY IV   \n",
       "\n",
       "                                          PlayerLine  \n",
       "0                                              ACT I  \n",
       "1                       SCENE I. London. The palace.  \n",
       "2  Enter KING HENRY, LORD JOHN OF LANCASTER, the ...  \n",
       "3             So shaken as we are, so wan with care,  \n",
       "4         Find we a time for frighted peace to pant,  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Shakespeare_data.csv', nrows = 50000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "['ACT I', 'SCENE I. London. The palace.', 'Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR WALTER BLUNT, and others']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "corpus = []\n",
    "\n",
    "with open('Shakespeare_data.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    next(reader)        # to pass first row,header\n",
    "    for row in reader:\n",
    "        corpus.append(row[5])\n",
    "\n",
    "corpus = corpus[:50000]       \n",
    "print(len(corpus))\n",
    "print(corpus[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def text_cleaner(text):\n",
    "    text = \"\".join(car for car in text if car not in string.punctuation).lower()\n",
    "    text = text.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return text\n",
    "\n",
    "corpus = [text_cleaner(line) for line in corpus[:50000]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3759"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization is the process of splitting up a text into a list of individual words, or tokens.\n",
    "# corpus is too big if you try with all data, you can see this message\n",
    "corpus = corpus[:3000]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index) + 1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input sequences using list of tokens\n",
    "input_sequences =[]\n",
    "\n",
    "for sentence in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Create predictors and label\n",
    "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "# Create one-hot encoding of the labels\n",
    "label = tf.keras.utils.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "628/628 [==============================] - 9s 13ms/step - loss: 6.7732 - accuracy: 0.0417\n",
      "Epoch 2/5\n",
      "628/628 [==============================] - 8s 13ms/step - loss: 6.1057 - accuracy: 0.0647\n",
      "Epoch 3/5\n",
      "628/628 [==============================] - 8s 13ms/step - loss: 5.6902 - accuracy: 0.0922\n",
      "Epoch 4/5\n",
      "628/628 [==============================] - 8s 13ms/step - loss: 5.3013 - accuracy: 0.1100\n",
      "Epoch 5/5\n",
      "628/628 [==============================] - 8s 13ms/step - loss: 4.8953 - accuracy: 0.1331\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build and compile the GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(GRU(150))\n",
    "model.add(Dense(total_words, activation='relu'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(predictors, label, epochs=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "help me in this same\n",
      " camp of shrewsbury\n",
      " and the day of the\n",
      " day\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"help me in this\"\n",
    "next_words = 10\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    c = np.argmax(predicted, axis = 1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == c:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "    if len(seed_text) % 10 == 0:\n",
    "        seed_text += '\\n'\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love all, trust a few flocks of the day of the day\n",
      " of the day\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Love all, trust a few\"\n",
    "next_words = 10\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    c = np.argmax(predicted, axis = 1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == c:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "    if len(seed_text) % 10 == 0:\n",
    "        seed_text += '\\n'\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 17s]\n",
      "val_accuracy: 0.03560756891965866\n",
      "\n",
      "Best val_accuracy So Far: 0.09138446301221848\n",
      "Total elapsed time: 00h 01m 28s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Best Hyperparameters:\n",
      "<keras_tuner.engine.hyperparameters.hyperparameters.HyperParameters object at 0x7ff028300410>\n",
      "Epoch 1/5\n",
      "502/502 [==============================] - 7s 11ms/step - loss: 6.8150 - accuracy: 0.0406 - val_loss: 6.6352 - val_accuracy: 0.0632\n",
      "Epoch 2/5\n",
      "502/502 [==============================] - 5s 11ms/step - loss: 6.0683 - accuracy: 0.0832 - val_loss: 6.5018 - val_accuracy: 0.0876\n",
      "Epoch 3/5\n",
      "502/502 [==============================] - 6s 11ms/step - loss: 5.6277 - accuracy: 0.1179 - val_loss: 6.5354 - val_accuracy: 0.0951\n",
      "Epoch 4/5\n",
      "502/502 [==============================] - 5s 11ms/step - loss: 5.2273 - accuracy: 0.1444 - val_loss: 6.6057 - val_accuracy: 0.0941\n",
      "Epoch 5/5\n",
      "502/502 [==============================] - 6s 11ms/step - loss: 4.8444 - accuracy: 0.1727 - val_loss: 6.7230 - val_accuracy: 0.0986\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 29, 96)            360864    \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 96)                55872     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3759)              364623    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 781359 (2.98 MB)\n",
      "Trainable params: 781359 (2.98 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, hp.Int('embedding_dim', min_value=32, max_value=128, step=32), input_length=max_sequence_len-1))\n",
    "    model.add(GRU(units=hp.Int('gru_units', min_value=32, max_value=128, step=32), dropout=hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(total_words, activation=hp.Choice('dense_activation',values=['relu','sigmoid','softmax']),kernel_initializer='he_normal'))\n",
    "    model.compile(optimizer='adam', loss=hp.Choice('loss_fn',values=['binary_crossentropy','categorical_crossentropy']), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize Keras Tuner RandomSearch\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    directory='tuner_dir',\n",
    "    project_name='lstm_sentiment'\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(predictors, label, validation_split=0.2, epochs=3)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hps)\n",
    "\n",
    "# Build the final model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(predictors, label, validation_split=0.2, epochs=5, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen embedding dimension: 96\n",
      "Chosen number of LSTM units: 96\n",
      "Chosen Activation Function: softmax\n",
      "Chosen Loss Function: categorical_crossentropy\n"
     ]
    }
   ],
   "source": [
    "# Print the chosen activation function and loss function\n",
    "best_activation = best_hps.get('dense_activation')\n",
    "best_loss_function = best_hps.get('loss_fn')\n",
    "best_em_dim = best_hps.get('embedding_dim')\n",
    "best_units = best_hps.get('gru_units')\n",
    "print(\"Chosen embedding dimension:\", best_em_dim)\n",
    "print(\"Chosen number of GRU units:\", best_units)\n",
    "print(\"Chosen Activation Function:\", best_activation)\n",
    "print(\"Chosen Loss Function:\", best_loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(GRU(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(predictors, label, epochs=5,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"help me in this\"\n",
    "next_words = 10\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    c = np.argmax(predicted, axis = 1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == c:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "    if len(seed_text) % 10 == 0:\n",
    "        seed_text += '\\n'\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"Love all, trust a few\"\n",
    "next_words = 10\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    c = np.argmax(predicted, axis = 1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == c:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "    if len(seed_text) % 10 == 0:\n",
    "        seed_text += '\\n'\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
